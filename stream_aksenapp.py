# -*- coding: utf-8 -*-
"""stream_aksenapp

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rkkiJ8OjDkTlOOP5mc3p5n20ld9HgEu3
"""

import streamlit as st
import numpy as np
import pandas as pd
import librosa
import soundfile as sf
import tensorflow as tf
import tempfile
import os
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.preprocessing import OneHotEncoder
import plotly.graph_objects as go
import plotly.express as px

# Set page config
st.set_page_config(
    page_title="Few-Shot Accent Detection",
    page_icon="üéôÔ∏è",
    layout="wide"
)

# Title and description
st.title("üéôÔ∏è Few-Shot Accent Detection with Multi-Task Learning")
st.markdown("""
    This application detects Indonesian accents from audio files using Few-Shot Learning.
    The model also predicts gender and province as additional tasks.
""")

# Sidebar for model info
st.sidebar.header("Model Information")
st.sidebar.info("""
    **Model Type:** Prototypical Network with Multi-Task Learning

    **Tasks:**
    1. Accent Classification (Few-Shot)
    2. Gender Prediction
    3. Province Prediction

    **Input:** Audio file (.wav)
    **Output:** Accent, Gender, Province predictions
""")

# Load models and encoders (adjust paths as needed)
@st.cache_resource
def load_resources():
    """Load model and preprocessing resources"""
    try:
        # Load model
        model_path = 'model.keras'
        custom_objects = {"PrototypicalNetwork": PrototypicalNetwork}
        model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)

        # Load encoders and scalers (you'll need to save these from your training)
        # For demo purposes, we'll create dummy ones
        le_y = LabelEncoder()
        le_gender = LabelEncoder()
        le_provinsi = LabelEncoder()
        scaler_usia = StandardScaler()
        ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=False)

        return model, le_y, le_gender, le_provinsi, scaler_usia, ohe
    except Exception as e:
        st.error(f"Error loading resources: {e}")
        return None, None, None, None, None, None

# Define PrototypicalNetwork class (same as in training)
import keras
from tensorflow.keras import Model, layers

@keras.saving.register_keras_serializable()
class PrototypicalNetwork(Model):
    def __init__(self, embedding_model, **kwargs):
        super(PrototypicalNetwork, self).__init__(**kwargs)
        self.embedding = embedding_model

    def call(self, support_set, query_set, support_labels, n_way):
        # Calculate embeddings
        support_embeddings = self.embedding(support_set)
        query_embeddings = self.embedding(query_set)

        # Calculate prototypes per class
        prototypes = []
        for i in range(n_way):
            mask = tf.equal(support_labels, i)
            class_embeddings = tf.boolean_mask(support_embeddings, mask)
            prototype = tf.reduce_mean(class_embeddings, axis=0)
            prototypes.append(prototype)
        prototypes = tf.stack(prototypes)

        # Calculate Euclidean distances
        distances = []
        for q in query_embeddings:
            dist = tf.norm(prototypes - q, axis=1)
            distances.append(dist)
        distances = tf.stack(distances)

        # Convert distances to probabilities
        logits = -distances
        return logits

    def get_config(self):
        config = super(PrototypicalNetwork, self).get_config()
        config.update({
            "embedding_model": keras.saving.serialize_keras_object(self.embedding)
        })
        return config

    @classmethod
    def from_config(cls, config):
        embedding_config = config.pop("embedding_model")
        embedding_model = keras.saving.deserialize_keras_object(embedding_config)
        return cls(embedding_model, **config)

# Feature extraction function
def extract_features_from_audio(audio_path, sr=22050, n_mfcc=40, max_len=174):
    """Extract MFCC features from audio file"""
    try:
        # Load audio
        y, sr = librosa.load(audio_path, sr=sr)

        # Normalize amplitude
        y = librosa.util.normalize(y)

        # Extract MFCC
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=2048, hop_length=512)

        # Delta and Delta-Delta
        delta = librosa.feature.delta(mfcc)
        delta2 = librosa.feature.delta(mfcc, order=2)

        # Padding or truncating
        if mfcc.shape[1] < max_len:
            pad_width = max_len - mfcc.shape[1]
            mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')
            delta = np.pad(delta, ((0, 0), (0, pad_width)), mode='constant')
            delta2 = np.pad(delta2, ((0, 0), (0, pad_width)), mode='constant')
        else:
            mfcc = mfcc[:, :max_len]
            delta = delta[:, :max_len]
            delta2 = delta2[:, :max_len]

        # Stack into 3 channels
        features = np.stack([mfcc, delta, delta2], axis=-1)

        return features

    except Exception as e:
        st.error(f"Error extracting features: {e}")
        return None

# Prediction function
def predict_accent(model, audio_features, support_set, support_labels, n_way=5):
    """Make prediction using prototypical network"""
    # Convert to tensors
    support_tensor = tf.convert_to_tensor(support_set, dtype=tf.float32)
    query_tensor = tf.convert_to_tensor(audio_features[np.newaxis, ...], dtype=tf.float32)
    support_labels_tensor = tf.convert_to_tensor(support_labels, dtype=tf.int32)

    # Get logits
    logits = model.call(
        support_tensor,
        query_tensor,
        support_labels_tensor,
        n_way
    )

    # Convert to probabilities
    probabilities = tf.nn.softmax(logits).numpy()[0]

    return probabilities

# Main application
def main():
    # Load resources
    with st.spinner("Loading model and resources..."):
        model, le_y, le_gender, le_provinsi, scaler_usia, ohe = load_resources()

    if model is None:
        st.error("Failed to load model. Please check model files.")
        return

    # Create tabs
    tab1, tab2, tab3 = st.tabs(["üìä Upload Audio", "üéØ Multi-Task Results", "üìà Model Information"])

    with tab1:
        st.header("Upload Audio File")

        # File uploader
        uploaded_file = st.file_uploader(
            "Choose an audio file (.wav format)",
            type=["wav", "mp3", "m4a"],
            help="Upload an audio file to detect accent, gender, and province"
        )

        # Metadata input
        st.subheader("Additional Information (Optional)")
        col1, col2, col3 = st.columns(3)

        with col1:
            usia = st.number_input("Age", min_value=0, max_value=100, value=25)

        with col2:
            gender = st.selectbox("Gender", ["Male", "Female"])

        with col3:
            # Indonesian provinces
            provinces = [
                "Jawa Barat", "Jawa Tengah", "Jawa Timur", "Yogyakarta",
                "Banten", "Jakarta", "Bali", "Sumatera Utara", "Sumatera Barat"
            ]
            provinsi = st.selectbox("Province", provinces)

        # Display audio if uploaded
        if uploaded_file is not None:
            # Save uploaded file temporarily
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
                tmp_file.write(uploaded_file.getbuffer())
                tmp_path = tmp_file.name

            # Display audio player
            st.audio(uploaded_file, format="audio/wav")

            # Extract features
            with st.spinner("Extracting audio features..."):
                audio_features = extract_features_from_audio(tmp_path)

                if audio_features is not None:
                    st.success("‚úÖ Features extracted successfully!")

                    # Store in session state
                    st.session_state['audio_features'] = audio_features
                    st.session_state['metadata'] = {
                        'usia': usia,
                        'gender': gender,
                        'provinsi': provinsi
                    }

                    # Display feature info
                    st.info(f"**Feature shape:** {audio_features.shape}")

                    # Clean up temp file
                    os.unlink(tmp_path)

    with tab2:
        st.header("Multi-Task Prediction Results")

        if 'audio_features' not in st.session_state:
            st.warning("Please upload an audio file first.")
        else:
            # Create columns for results
            col1, col2, col3 = st.columns(3)

            # Mock predictions (replace with actual model inference)
            with st.spinner("Making predictions..."):
                # For demo - replace with actual model predictions
                accent_classes = ["Sunda", "Jawa_Tengah", "Jawa_Timur", "YogyaKarta", "Bali"]
                accent_probs = np.random.dirichlet(np.ones(5))
                predicted_accent = accent_classes[np.argmax(accent_probs)]

                gender_pred = "Male" if np.random.random() > 0.5 else "Female"
                gender_conf = np.random.uniform(0.7, 0.95)

                province_pred = st.session_state['metadata']['provinsi']
                province_conf = np.random.uniform(0.6, 0.9)

                # Display results
                with col1:
                    st.metric(
                        label="üé≠ Predicted Accent",
                        value=predicted_accent,
                        delta=f"{max(accent_probs)*100:.1f}% confidence"
                    )

                    # Accent probabilities chart
                    fig1 = go.Figure(data=[
                        go.Bar(
                            x=accent_classes,
                            y=accent_probs * 100,
                            marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']
                        )
                    ])
                    fig1.update_layout(
                        title="Accent Probabilities",
                        xaxis_title="Accent",
                        yaxis_title="Probability (%)",
                        height=300
                    )
                    st.plotly_chart(fig1, use_container_width=True)

                with col2:
                    st.metric(
                        label="üë§ Predicted Gender",
                        value=gender_pred,
                        delta=f"{gender_conf*100:.1f}% confidence"
                    )

                    # Gender confidence gauge
                    fig2 = go.Figure(go.Indicator(
                        mode="gauge+number",
                        value=gender_conf * 100,
                        title={'text': "Gender Confidence"},
                        gauge={
                            'axis': {'range': [0, 100]},
                            'bar': {'color': "#4ECDC4"},
                            'steps': [
                                {'range': [0, 50], 'color': "lightgray"},
                                {'range': [50, 100], 'color': "gray"}
                            ]
                        }
                    ))
                    fig2.update_layout(height=300)
                    st.plotly_chart(fig2, use_container_width=True)

                with col3:
                    st.metric(
                        label="üìç Predicted Province",
                        value=province_pred,
                        delta=f"{province_conf*100:.1f}% confidence"
                    )

                    # Province confidence
                    fig3 = go.Figure(go.Indicator(
                        mode="gauge+number",
                        value=province_conf * 100,
                        title={'text': "Province Confidence"},
                        gauge={
                            'axis': {'range': [0, 100]},
                            'bar': {'color': "#45B7D1"},
                            'steps': [
                                {'range': [0, 60], 'color': "lightgray"},
                                {'range': [60, 100], 'color': "gray"}
                            ]
                        }
                    ))
                    fig3.update_layout(height=300)
                    st.plotly_chart(fig3, use_container_width=True)

                # Display metadata
                st.subheader("üìã Input Metadata")
                metadata = st.session_state['metadata']
                meta_col1, meta_col2, meta_col3 = st.columns(3)

                with meta_col1:
                    st.info(f"**Age:** {metadata['usia']}")
                with meta_col2:
                    st.info(f"**Gender:** {metadata['gender']}")
                with meta_col3:
                    st.info(f"**Province:** {metadata['provinsi']}")

    with tab3:
        st.header("Model Architecture")

        # Model architecture description
        st.markdown("""
        ### Prototypical Network with Multi-Task Learning

        **Architecture Overview:**

        ```
        Input Audio ‚Üí MFCC Features ‚Üí Embedding Network ‚Üí Prototypes ‚Üí Predictions
                               ‚Üì
                        Metadata (Age, Gender, Province)
        ```

        **Embedding Network Layers:**
        1. Conv2D(128, 3x3) + ReLU
        2. MaxPooling2D(2x2)
        3. GlobalAveragePooling2D
        4. Dense(256) + ReLU + Dropout(0.3)
        5. Dense(128) + ReLU

        **Multi-Task Outputs:**
        - **Task 1:** Accent Classification (Few-Shot)
        - **Task 2:** Gender Prediction
        - **Task 3:** Province Prediction

        **Loss Function:**
        ```python
        Total Loss = w1 * Accent_Loss + w2 * Gender_Loss + w3 * Province_Loss
        ```

        **Training Parameters:**
        - Few-Shot: 5-way, 5-shot, 5-query
        - Learning Rate: 1e-4
        - Epochs: 50
        """)

        # Display feature dimensions
        if 'audio_features' in st.session_state:
            st.subheader("Current Audio Features")
            features = st.session_state['audio_features']

            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("MFCC Coefficients", features.shape[0])
            with col2:
                st.metric("Time Frames", features.shape[1])
            with col3:
                st.metric("Channels", features.shape[2])

# Run the app
if __name__ == "__main__":
    main()
